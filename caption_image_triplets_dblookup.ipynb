{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers  gdown torch numpy tqdm Pillow scikit-image gradio git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "X0jv4Xrr_UDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image Caption\n",
        "\n",
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "import clip\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as nnf\n",
        "import sys\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "import gradio as gr\n",
        "\n",
        "coco_weight = hf_hub_download(repo_id=\"akhaliq/CLIP-prefix-captioning-COCO-weights\", filename=\"coco_weights.pt\")\n",
        "\n",
        "N = type(None)\n",
        "V = np.array\n",
        "ARRAY = np.ndarray\n",
        "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
        "VS = Union[Tuple[V, ...], List[V]]\n",
        "VN = Union[V, N]\n",
        "VNS = Union[VS, N]\n",
        "T = torch.Tensor\n",
        "TS = Union[Tuple[T, ...], List[T]]\n",
        "TN = Optional[T]\n",
        "TNS = Union[Tuple[TN, ...], List[TN]]\n",
        "TSN = Optional[TS]\n",
        "TA = Union[T, ARRAY]\n",
        "\n",
        "\n",
        "D = torch.device\n",
        "CPU = torch.device('cpu')\n",
        "\n",
        "\n",
        "def get_device(device_id: int) -> D:\n",
        "    if not torch.cuda.is_available():\n",
        "        return CPU\n",
        "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
        "    return torch.device(f'cuda:{device_id}')\n",
        "\n",
        "\n",
        "CUDA = get_device\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: T) -> T:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) -1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    #@functools.lru_cache #FIXME\n",
        "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
        "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
        "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
        "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
        "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if prefix_length > 10:  # not enough memory\n",
        "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
        "        else:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self\n",
        "        \n",
        "\n",
        "\n",
        "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
        "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
        "\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = torch.ones(beam_size, device=device)\n",
        "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
        "    with torch.no_grad():\n",
        "        if embed is not None:\n",
        "            generated = embed\n",
        "        else:\n",
        "            if tokens is None:\n",
        "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                tokens = tokens.unsqueeze(0).to(device)\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "        for i in range(entry_length):\n",
        "            outputs = model.gpt(inputs_embeds=generated)\n",
        "            logits = outputs.logits\n",
        "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "            logits = logits.softmax(-1).log()\n",
        "            if scores is None:\n",
        "                scores, next_tokens = logits.topk(beam_size, -1)\n",
        "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
        "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
        "                if tokens is None:\n",
        "                    tokens = next_tokens\n",
        "                else:\n",
        "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
        "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "            else:\n",
        "                logits[is_stopped] = -float(np.inf)\n",
        "                logits[is_stopped, 0] = 0\n",
        "                scores_sum = scores[:, None] + logits\n",
        "                seq_lengths[~is_stopped] += 1\n",
        "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
        "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "                seq_lengths = seq_lengths[next_tokens_source]\n",
        "                next_tokens = next_tokens % scores_sum.shape[1]\n",
        "                next_tokens = next_tokens.unsqueeze(1)\n",
        "                tokens = tokens[next_tokens_source]\n",
        "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
        "                generated = generated[next_tokens_source]\n",
        "                scores = scores_sum_average * seq_lengths\n",
        "                is_stopped = is_stopped[next_tokens_source]\n",
        "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
        "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "    scores = scores / seq_lengths\n",
        "    output_list = tokens.cpu().numpy()\n",
        "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
        "    order = scores.argsort(descending=True)\n",
        "    output_texts = [output_texts[i] for i in order]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt=None,\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.8,\n",
        "        temperature=1.,\n",
        "        stop_token: str = '.',\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    filter_value = -float(\"Inf\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "            if embed is not None:\n",
        "                generated = embed\n",
        "            else:\n",
        "                if tokens is None:\n",
        "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
        "                    tokens = tokens.unsqueeze(0).to(device)\n",
        "\n",
        "                generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "\n",
        "                outputs = model.gpt(inputs_embeds=generated)\n",
        "                logits = outputs.logits\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                                                    ..., :-1\n",
        "                                                    ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
        "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "                if tokens is None:\n",
        "                    tokens = next_token\n",
        "                else:\n",
        "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
        "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
        "                if stop_token_index == next_token.item():\n",
        "                    break\n",
        "\n",
        "            output_list = list(tokens.squeeze().cpu().numpy())\n",
        "            output_text = tokenizer.decode(output_list)\n",
        "            generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]\n",
        "    \n",
        "is_gpu = False \n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "  \n",
        "prefix_length = 10\n",
        "model = ClipCaptionModel(prefix_length)\n",
        "model_path = coco_weight\n",
        "model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
        "model = model.eval() \n",
        "device = CUDA(0) if is_gpu else \"cpu\"\n",
        "model = model.to(device)\n",
        "use_beam_search = False\n",
        "\n",
        "def inference(image):\n",
        "  pil_image = PIL.Image.fromarray(image)  \n",
        "  image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "  with torch.no_grad():\n",
        "      prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "      prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "  if use_beam_search:\n",
        "      generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
        "  else:\n",
        "      generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "  return generated_text_prefix\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tCYe37Kh__lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgK6-LPV-OdA"
      },
      "source": [
        "#@title Standford NLP setup\n",
        "# Install stanza; note that the prefix \"!\" is not needed if you are running in a terminal\n",
        "!pip install stanza\n",
        "\n",
        "# Import stanza\n",
        "import stanza\n",
        "\n",
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "# This'll take several minutes, depending on the network speed\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)\n",
        "\n",
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
        "\n",
        "from stanza.server import CoreNLPClient\n",
        "client = CoreNLPClient(\n",
        "    annotators=\"tokenize ssplit lemma pos ner depparse natlog openie\".split(), \n",
        "    memory='4G', \n",
        "    endpoint='http://localhost:9001',\n",
        "    be_quiet=True)\n",
        "\n",
        "client.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy spacy-dbpedia-spotlight\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "2TVLzjjOK4OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy_dbpedia_spotlight\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "nlp.add_pipe('dbpedia_spotlight')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_HVXLD5K88p",
        "outputId": "d363f789-362e-40f4-a1d4-53f82122af76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy_dbpedia_spotlight.entity_linker.EntityLinker at 0x7f4642027750>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_PATH = \"/content/images.jpeg\"\n",
        "image = io.imread(IMG_PATH)\n",
        "text = inference(image)\n",
        "print(text)\n",
        "# doc = nlp(text)\n",
        "# see the entities\n",
        "# print('Entities', [(ent.text, ent.label_, ent.kb_id_) for ent in doc.ents])\n",
        "# inspect the raw data from DBpedia spotlight\n",
        "# print(doc.ents[0]._.dbpedia_raw_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC9wlmKDMuNy",
        "outputId": "6529ba90-498f-4f89-fad0-428f49c44e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man drinking a glass of orange juice.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJsuO6D8D05q"
      },
      "source": [
        "## 2. Annotating Text with CoreNLP Interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = client.annotate(text)\n",
        "for x in document.sentence:\n",
        "   for triple in x.openieTriple:\n",
        "     print(f\"({triple.subject}, {triple.relation}, {triple.object})\")\n",
        "     doc = nlp(triple.subject)\n",
        "     print([(ent.text, ent.label_, ent.kb_id_) for ent in doc.ents])\n",
        "\n",
        "# for i, sent in enumerate(document.sentence):\n",
        "#     print(\"[Sentence {}]\".format(i+1))\n",
        "#     for t in sent.token:\n",
        "#         print(\"{:12s}\\t{:12s}\\t{:6s}\\t{}\".format(t.word, t.lemma, t.pos, t.ner))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8_aMSd5pGW_",
        "outputId": "c65c902f-a257-4185-cef4-8af298fc6a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(man, drinking, glass)\n",
            "[]\n",
            "(man, drinking, glass of orange juice)\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"A man walks up to his car holding some groceries. A dog from another car starts barking at the man and the man drops the bags. The man startles the dog. The dog opens the car window. The man breaks the window and gets in the car leaving his groceries. Theres a monster in the back of his car in the final scene.\"\n",
        "text =\"Two buffalos are standing next to a water. They notice a floating thing. They start arguing about the origin of it. One is saying that it is a crocodile, the other that it is a log. One of them starts checking it by throwing a rock at it and punching it with a stick. In the end, when the buffalo climbs on it the crocodile attacks the buffalo and eats him. The one saying that it was a log survived.\"\n",
        "\n",
        "\n",
        "def camelCase(st):\n",
        "    output = ''.join(x for x in st.title() if x.isalnum())\n",
        "    return output[0].lower() + output[1:]\n",
        "\n",
        "document = client.annotate(text)\n",
        "\n",
        "triples = [(triple.subject, triple.relation, triple.object) for x in document.sentence for triple in x.openieTriple]\n",
        "triples = [(s, camelCase(r), o) for s, r, o in triples]\n",
        "print(\"\\n\".join(str(t) for t in triples))"
      ],
      "metadata": {
        "id": "yOkdVxbcuWVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e8e496-3119-445b-ba75-834dd5d058b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('buffalos', 'areStandingNextTo', 'water')\n",
            "('buffalos', 'areStandingTo', 'water')\n",
            "('Two buffalos', 'areStandingTo', 'water')\n",
            "('Two buffalos', 'areStandingNextTo', 'water')\n",
            "('They', 'notice', 'thing')\n",
            "('They', 'notice', 'floating thing')\n",
            "('They', 'start', 'arguing')\n",
            "('They', 'arguingAbout', 'origin')\n",
            "('They', 'start', 'arguing about origin')\n",
            "('They', 'start', 'arguing about origin of it')\n",
            "('They', 'arguingAbout', 'origin of it')\n",
            "('rock', 'punching', 'it')\n",
            "('buffalo', 'climbsOn', 'it')\n",
            "('crocodile', 'attacks', 'buffalo')\n",
            "('crocodile', 'eats', 'him')\n",
            "('crocodile', 'attacksBuffaloIn', 'end')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKQg--M-vxpz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}